# Experiments

We provide here the needed materials and a step-by-step guide to reproduce the experiments we carried out in order to build, train and evaluate the meta-learners.

Here are the different steps:
1. [Retrieving the datasets](#retrieving-the-datasets)
2. [Computing meta-features](#computing-meta-features)
3. [Computing similarity measures pairs performances](#computing-similarity-measures-pairs-performances)
4. [Training and evaluation of the meta-learners](#training-and-evaluation-of-the-meta-learners)

## Meta-dataset creation

### Retrieving the datasets
The datasets were retrieved from the [OpenML platform](https://www.openml.org/). We consider only labelled mixed datasets without missing values. For each dataset, 10 augmented datasets are generated by down-sampling on rows and columns.
The code for retrieving these datasets is available in [find_datasets.py](find_datasets.py)

``` bash
python find_dataset.py -o path/to/your/output/directory
```
We do not download the data, but only retrieve their id. The results are saved in a file named `infos_found_datasets.pickle` in the output directory. You can see our results after running the script in [data](data/). Here is the format of the file:

``` json
{
    "<id>": {
        "<id>_0": {
            "samples": [...],
            "num_columns": [...],
            "cat_columns": [...]
        },
        
        ...,

        "<id>_10": {
            "samples": [...],
            "num_columns": [...],
            "cat_columns": [...]
        }
    },

    ...
}
```
`<id>` is the id of the dataset in OpenML. `<id>_<k>` is the $k^{th}$ augmented dataset generated from the original dataset. $k=0$ correspond to the original dataset itself. `samples` is a list containing the indices of the observations of the original datasets that are in the augmented dataset. Similarly, `num_columns` and `cat_columns` are lists containing the indices of the numeric and categorical attributes of the original datasets that are in the augmented dataset.

### Computing meta-features
We compute the meta-features of each dataset. We have implemented 61 meta-features (refer to the paper for detailed description). The script for computing these meta-features is given in [generate_meta_features.py](generate_meta_features.py). 

This script take as argument the file containing the information about the retrieved datasets (`-i`) described in the previous section [previous section](#retrieving-the-datasets). It also take as input a file containing manually selected datasets from the retrieved dataset (`-s`), see [selected_datasets.csv](data/selected_datasets.csv). This manual filtering was necessary due to the presence of redundant datasets in OpenML. The last parameter is the output directory where the results will be saved (`-o`).

Here is an example for running the script:
``` bash
python generate_meta_features.py -i data/infos_found_datasets.pickle -s data/selected_datasets.csv -o data
```
The results are saved with the name `meta-features.csv`. You can see our results after running the script in [data](data/). Here is the format of the file:

``` csv
augmented_id, meta-feature_1, ..., <meta-feature_61>
222_0, 0.1, ..., 0.005
222_1, 0.1, ..., 0.005
```
### Computing similarity measures pairs performances
For each considered clustering algorithm, we define the performance of a similarity measures pair $s$ on a given a dataset $\mathcal{D}$ as the highest accuracy (i.e. with the optimal parameters of the algorithm) we can achive on $\mathcal{D}$ using $s$. 

The script [benchmark.py](benchmark.py/) allows to run a given clustering algorithm on all dataset, with all similarity measures pairs while searching for the optimal parameters at each run. It outputs 2 JSON files, one containing the performances of the similarity measures pairs for for each dataset and another containing.

Here is an example with the HAverage algorithm:
``` bash
python benchmark.py -i data/infos_found_datasets.pickle -s data/selected_datasets.csv -o data -a haverage -e acc
```

- `-a` option specifies the clustering algorithm. The following algorithms are availble: [`haverage`, `fasterpam`, `sfkm`, `kprototypes`, `spectral`] for Hierarchical Clustering with average linkage, FasterPAM and SFKM versions of K-Medoids, K-Prototypes and Spectral Clustering respectively.

- `-e` option corresponds to the clustering evaluation metric from [`acc`, `ari`, `sil`] for Accuracy, Adjusted Rand Index and Silhouette scores respectively.

The obtained results for `haverage`, `fasterpam` version of K-Medoids are available in [data/HAverage/](data/HAverage/) and [data/KPrototypes/](data/KPrototypes/) respectively.

## Training and evaluation of the meta-learners
Once the meta-datasets are created, They can be used to train the meta-learners. The code for the training and evaluation of the meta-learners using a Leave-One-Out (LOO) procedure is given in [train_and_eval.py](train_and_eval.py). In this procedure, we realize $N_{OD}$ (the number of original datasets) iterations, such that at each iteration, one original dataset is selected for testing while all remaining datasets except the augmentations of the selected dataset, are used for training. During the training, the hyper-parameters of the different meta-learners are defined using a grid search
cross-validation strategy.

Obtained results for the two considered algorithms can be found in [data/HAverage/](data/HAverage/) and [data/KPrototypes/](data/KPrototypes/) respectively. The results contain the predictions of the meta-learners on each original dataset when it was used as a test dataset during the LOO precedure. So, the results allow to evaluate the generalization performances of the meta-learners on the original datasets.
The performances showed in the paper are based on these results. By the way you can reproduce all the graphics pesented in the paper using the notebook [paper.ipynb](paper.ipynb).

Finally we made the meta-learners available in [this folder](../models/), but before making them available, they have been re-trained on the entire datasets (see [train_and_save.py](train_and_save.py)).