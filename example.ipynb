{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a practical use case\n",
    "\n",
    "We present in this notebook a practical use case. Concretely, we will:\n",
    "1. Create a synthetic mixed dataset\n",
    "2. Compute the meta-features of the dataset\n",
    "3. Load one of the pre-trained meta-learners (namely _KNN_ for K-Prototypes algorithm) and the scaler\n",
    "4. Use the loaded meta-learner to predict the ranking of similarity measures pair.\n",
    "5. Run the K-Prototypes algorithm with the 5 top ranked pairs and compare their results with the literature baseline (*squared euclidean distance*, *hamming distance*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mixed_metrics import get_valid_similarity_pairs\n",
    "from meta_features import compute_meta_features\n",
    "from sklearn.datasets import make_blobs\n",
    "from mixed_metrics import WeightedAverage\n",
    "from sklearn.preprocessing import OneHotEncoder, minmax_scale\n",
    "from base_metrics import get_available_metrics, get_metric\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from experiments.utils import get_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a synthetic dataset\n",
    "\n",
    "We first create a numeric dataset using the `make_blobs` function in _scikit-learn_. Then, we transform some numeric attributes into categorical ones by discretizing their values.\n",
    "\n",
    "We also create a one-hot encoding representation of the categorical attributes that will be used by binary similarity measures. One-hot encoding consist in transforming each categorical attribute in several binary attributes, each one corresponding to one category of the transformed categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x, n):\n",
    "    \"\"\"Discretize a list/array x by dividing its values into n intervals. \n",
    "    Each value in x is then associated with a category corresponding to one of the intervals.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list / 1D array\n",
    "        The values to discretize\n",
    "    n : int\n",
    "        The number of categories\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array\n",
    "        The discretized values\n",
    "    \"\"\"\n",
    "    eps=1e-5\n",
    "    bins = np.linspace(min(x), max(x)+eps, n+1)\n",
    "    x_discrete = np.digitize(x, bins) - 1\n",
    "    permutation = np.random.permutation(n)\n",
    "    return permutation[x_discrete]\n",
    "\n",
    "# Create a synthetic dataset with the following parameters\n",
    "n_clusters = 5\n",
    "n_att = 3\n",
    "c_att = 7\n",
    "n_feats = n_att + c_att\n",
    "n_samples = 200\n",
    "\n",
    "# We use the make_blobs function in sklearn to create an initial dataset with the total number of features\n",
    "X, y = make_blobs(\n",
    "    centers=n_clusters,\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_feats,\n",
    "    cluster_std=5,\n",
    "    random_state=0\n",
    ")\n",
    "y = y.flatten()\n",
    "\n",
    "# The we separate the numeric and categorical parts\n",
    "Xnum = X[:, :n_att]\n",
    "Xcat = np.zeros(shape=(n_samples, c_att))\n",
    "\n",
    "# Finally we discretize the categorical attributes\n",
    "for j in range(c_att):\n",
    "    n_cat = np.random.randint(2, 10)\n",
    "    Xcat[:, j] = discretize(X[:, n_att+j], n_cat)\n",
    "\n",
    "# We create the one hot encoding representation of the Xcat which will be used with binary similarity measures\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Xdummy = enc.fit_transform(Xcat).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the meta-feature vector of the dataset\n",
    "\n",
    "To compute the meta-features vector, you can simply use the `compute_meta_features` function in [meta_features.py](meta_features.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Normalize the numeric part before computing the meta-features or performing clustering\n",
    "Xnum = minmax_scale(Xnum)\n",
    "\n",
    "# create the meta-features vector of your dataset\n",
    "mf_vector = compute_meta_features(Xnum, Xcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model and the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the KNN model\n",
    "with open(\"models/KPrototypes/KNN.pickle\", \"rb\") as f:\n",
    "    ranker = pickle.load(f)\n",
    "\n",
    "# load the scaler. The scaler is used to transform the meta-feature vector before passing it to the meta-learner.\n",
    "with open(\"models/KPrototypes/scaler.pickle\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the ranking of the similarity measures pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canberra_sokalsneath', 'divergence_sokalsneath', 'divergence_jaccard', 'canberra_jaccard', 'canberra_dice']\n"
     ]
    }
   ],
   "source": [
    "# get the valid similarity measures pairs the dataset\n",
    "valid_pairs = get_valid_similarity_pairs(Xnum, Xcat)\n",
    "\n",
    "# predict the ranks/scores of all similarity measures pairs\n",
    "y_pred = ranker.predict(scaler.transform([mf_vector]))[0]\n",
    "\n",
    "# get a ranked list of similarity measures pairs\n",
    "ranked_pairs = ranker.similarity_pairs_[np.argsort(-y_pred)]\n",
    "\n",
    "# keep only valid similarity measures pairs for your dataset\n",
    "ranked_pairs = [sim_pair for sim_pair in ranked_pairs if sim_pair in valid_pairs]\n",
    "print(ranked_pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run K-Prototypes using the top ranked pairs and compare to the literature baseline\n",
    "\n",
    "We run K-Prototypes using the literature baseline and the 5 top ranked similarity measures pairs. For each pair, we run the algorithm using different weights for the combination of the two similarity measures of the pair, and show the clustering accuracy score obtained with the best weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the literature baseline: 0.55\n"
     ]
    }
   ],
   "source": [
    "literature_baseline = \"sqeuclidean_hamming\"\n",
    "\n",
    "X = np.c_[Xnum, Xcat]\n",
    "num_metric = get_metric(literature_baseline.split(\"_\")[0]).fit(Xnum)\n",
    "cat_metric = get_metric(literature_baseline.split(\"_\")[0]).fit(Xcat)\n",
    "weights = np.concatenate((np.linspace(0, 1, 6), np.arange(2, 10)))\n",
    "scores = []\n",
    "# We run the algorithm for different weight\n",
    "for i, gamma in enumerate(weights):\n",
    "    kp = KPrototypes(\n",
    "        n_clusters=n_clusters,\n",
    "        gamma=gamma, \n",
    "        num_dissim=num_metric.flex,\n",
    "        cat_dissim=cat_metric.flex,\n",
    "        random_state=0,\n",
    "        n_init=10,\n",
    "        init='huang',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clusters = kp.fit_predict(X, categorical=list(range(Xnum.shape[1], X.shape[1])))\n",
    "    scores.append(get_score(y, clusters, eval_metric=\"acc\"))\n",
    "\n",
    "best_score = max(scores)\n",
    "print(\"Score of the literature baseline:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of the 5 top ranked similarity measure pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>canberra_sokalsneath</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>divergence_sokalsneath</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divergence_jaccard</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canberra_jaccard</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>canberra_dice</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name  Score\n",
       "1    canberra_sokalsneath  0.620\n",
       "2  divergence_sokalsneath  0.615\n",
       "3      divergence_jaccard  0.615\n",
       "4        canberra_jaccard  0.620\n",
       "5           canberra_dice  0.620"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores = {}\n",
    "# For each of the 5 top ranked similarity measures pairs\n",
    "for k, pair_name in enumerate(ranked_pairs[:5]):\n",
    "    # print(f\"{pair_name}:\", end=\" \")\n",
    "    X = np.c_[Xnum, Xcat] if pair_name.split(\"_\")[1] in \\\n",
    "        get_available_metrics(data_type=\"categorical\") else np.c_[Xnum, Xdummy]\n",
    "    num_metric = get_metric(pair_name.split(\"_\")[0]).fit(Xnum)\n",
    "    cat_metric = get_metric(pair_name.split(\"_\")[0]).fit(X[:, Xnum.shape[1]:])\n",
    "    weights = list(np.linspace(0, 0.9, 10)) + list(np.linspace(1, 10, 19))\n",
    "    scores = []\n",
    "    \n",
    "    # We run the algorithm for different weights\n",
    "    for i, gamma in enumerate(weights):\n",
    "        kp = KPrototypes(\n",
    "            n_clusters=n_clusters,\n",
    "            gamma=gamma, \n",
    "            num_dissim=num_metric.flex,\n",
    "            cat_dissim=cat_metric.flex,\n",
    "            random_state=0,\n",
    "            n_init=10,\n",
    "            init='huang',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clusters = kp.fit_predict(X, categorical=list(range(Xnum.shape[1], X.shape[1])))\n",
    "        scores.append(get_score(y, clusters, eval_metric=\"acc\"))\n",
    "\n",
    "    # And store the best accuracy score\n",
    "    best_score = max(scores)\n",
    "    # print(best_score)\n",
    "    best_scores[k+1] = {\n",
    "        \"Name\": pair_name,\n",
    "        \"Score\": best_score\n",
    "    }\n",
    "df = pd.DataFrame.from_dict(best_scores, orient=\"index\")\n",
    "print(\"Scores of the 5 top ranked similarity measure pairs\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in this use case, the top ranked pairs has an accuracy of 0.62 while the literature baseline has only 0.55"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
